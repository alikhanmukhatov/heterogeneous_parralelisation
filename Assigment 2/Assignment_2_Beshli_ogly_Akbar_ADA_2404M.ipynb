{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a14c1eb-c781-403f-a3f6-67b4f3a97659",
      "metadata": {
        "id": "4a14c1eb-c781-403f-a3f6-67b4f3a97659"
      },
      "source": [
        "# Задача 1. Введение в гетерогенную параллелизацию\n",
        "\n",
        "    Объясните, что такое гетерогенная параллелизация.\n",
        "        В ответе раскройте следующие аспекты:\n",
        "        1. различия между параллельными вычислениями на CPU и GPU;\n",
        "        2. преимущества гетерогенной параллелизации;\n",
        "        3. примеры реальных приложений, в которых используется гетерогенная\n",
        "        параллелизация.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1dd501",
      "metadata": {
        "id": "aa1dd501"
      },
      "source": [
        "### 1) Гетерогенная параллелизация\n",
        "\n",
        "    Гетерогенная параллелизация — это подход к выполнению вычислительных задач, при котором одновременно используются разные типы вычислительных устройств в одной системе,\n",
        "    например центральный процессор (CPU) и графический процессор (GPU).\n",
        "    Основная цель — задействовать сильные стороны каждого устройства для повышения производительности и эффективности вычислений.\n",
        "\n",
        "### 2) Различия между параллельными вычислениями на CPU и GPU\n",
        "\n",
        "    CPU имеет относительно небольшое число мощных ядер, оптимизированных для сложной логики, ветвлений и последовательного выполнения инструкций. GPU содержит тысячи мелких ядер и предназначен для массово-параллельных операций над большими массивами данных.\n",
        "\n",
        "    CPU: подходит для операций с интенсивной логикой, ветвлениями и динамическим потоком управления.\n",
        "\n",
        "    GPU: эффективен при однотипных численных операциях, например в линейной алгебре, обработке изображений или обучении нейронных сетей.\n",
        "\n",
        "    Использование только CPU или только GPU часто приводит к неоптимальному распределению ресурсов и снижению производительности.\n",
        "\n",
        "### 3) Преимущества гетерогенной параллелизации\n",
        "\n",
        "    1. Максимальная эффективность ресурсов — CPU выполняет управляющие задачи, а GPU обрабатывает массивные однотипные данные.\n",
        "\n",
        "    2. Сокращение времени выполнения — распределение задач по устройствам ускоряет обработку больших объёмов данных.\n",
        "\n",
        "    3. Гибкость и масштабируемость — задачи распределяются в зависимости от архитектуры устройств и объёма данных.\n",
        "\n",
        "    4. Энергетическая эффективность — позволяет уменьшить энергопотребление на единицу вычислений.\n",
        "\n",
        "### 4) Примеры реальных приложений\n",
        "\n",
        "        1) DEGIMA — суперкомпьютер для астрофизики\n",
        "        Использует CPU и GPU для моделирования N‑body систем, например, движения звёзд и галактик.\n",
        "        CPU управляет задачами и распределяет работу, GPU ускоряет вычисления сил взаимодействия между телами\n",
        "        Cсылка: https://en.wikipedia.org/wiki/DEGIMA\n",
        "\n",
        "        2) Exscalate4Cov — виртуальный скрининг лекарств\n",
        "        Проект для ускоренного поиска потенциальных лекарств против COVID‑19.\n",
        "        CPU отвечает за организацию симуляций, GPU выполняет массовые расчёты взаимодействий молекул с белками.\n",
        "        Cсылка: https://en.wikipedia.org/wiki/Exscalate4Cov\n",
        "\n",
        "        3) GAMER — GPU‑ускоренные астрофизические симуляции\n",
        "        Система для моделирования гидродинамических и гравитационных процессов в космических объектах.\n",
        "        CPU управляет логикой симуляции, GPU выполняет параллельные вычисления физических полей.\n",
        "        Cсылка: https://arxiv.org/abs/0907.3390"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d24ed29-9e44-410b-ae00-9103c34c3204",
      "metadata": {
        "id": "6d24ed29-9e44-410b-ae00-9103c34c3204"
      },
      "source": [
        "# Задача 2. Работа с массивами и OpenMP\n",
        "\n",
        "    Реализуйте программу на C++, которая:\n",
        "        1. Создаёт массив из 10 000 случайных чисел.\n",
        "        2. Находит минимальное и максимальное значения массива:\n",
        "            o в последовательной реализации;\n",
        "            o с использованием OpenMP для параллельной обработки.\n",
        "        3. Сравнивает время выполнения обеих реализаций и формулирует выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2157293-ad11-4549-a0ad-77bd0b9f8437",
      "metadata": {
        "id": "b2157293-ad11-4549-a0ad-77bd0b9f8437",
        "outputId": "cd3d86c7-4ccc-4b3b-b1c9-5e71a870aa08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting task2.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile task2.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <cstdlib>      // rand, srand\n",
        "#include <ctime>        // time\n",
        "#include <chrono>       // измерение времени\n",
        "#include <omp.h>        // OpenMP\n",
        "\n",
        "using namespace std;\n",
        "using namespace chrono;\n",
        "\n",
        "int main() {\n",
        "    const int N = 10000;   // размер массива\n",
        "\n",
        "    // Динамическое выделение памяти под массив\n",
        "    int* arr = new int[N];\n",
        "\n",
        "    // Инициализация генератора случайных чисел\n",
        "    srand(time(nullptr));\n",
        "\n",
        "    // Заполнение массива случайными числами от 1 до 100\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        arr[i] = rand() % 100 + 1;\n",
        "    }\n",
        "\n",
        "\n",
        "    // Последовательный поиск min и max\n",
        "    // ===============================\n",
        "\n",
        "    int min_seq = arr[0];\n",
        "    int max_seq = arr[0];\n",
        "\n",
        "    auto start_seq = high_resolution_clock::now();\n",
        "\n",
        "    for (int i = 1; i < N; i++) {\n",
        "        if (arr[i] < min_seq)\n",
        "            min_seq = arr[i];\n",
        "        if (arr[i] > max_seq)\n",
        "            max_seq = arr[i];\n",
        "    }\n",
        "\n",
        "    auto end_seq = high_resolution_clock::now();\n",
        "    double time_seq =\n",
        "        duration<double, milli>(end_seq - start_seq).count();\n",
        "\n",
        "\n",
        "    // Параллельный поиск min и max (OpenMP)\n",
        "    // ===============================\n",
        "\n",
        "    int min_par = arr[0];\n",
        "    int max_par = arr[0];\n",
        "\n",
        "    auto start_par = high_resolution_clock::now();\n",
        "\n",
        "#pragma omp parallel for reduction(min:min_par) reduction(max:max_par)\n",
        "    for (int i = 1; i < N; i++) {\n",
        "        if (arr[i] < min_par)\n",
        "            min_par = arr[i];\n",
        "        if (arr[i] > max_par)\n",
        "            max_par = arr[i];\n",
        "    }\n",
        "\n",
        "    auto end_par = high_resolution_clock::now();\n",
        "    double time_par =\n",
        "        duration<double, milli>(end_par - start_par).count();\n",
        "\n",
        "\n",
        "    // Вывод результатов\n",
        "    // ===============================\n",
        "\n",
        "    cout << \"Последовательная реализация:\\n\";\n",
        "    cout << \"Минимум: \" << min_seq << endl;\n",
        "    cout << \"Максимум: \" << max_seq << endl;\n",
        "    cout << \"Время выполнения: \" << time_seq << \" мс\\n\\n\";\n",
        "\n",
        "    cout << \"Параллельная реализация (OpenMP):\\n\";\n",
        "    cout << \"Минимум: \" << min_par << endl;\n",
        "    cout << \"Максимум: \" << max_par << endl;\n",
        "    cout << \"Время выполнения: \" << time_par << \" мс\\n\";\n",
        "\n",
        "    // Освобождение памяти\n",
        "    delete[] arr;\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a17f573f-f7ca-48a4-a773-2206427aee28",
      "metadata": {
        "id": "a17f573f-f7ca-48a4-a773-2206427aee28",
        "outputId": "8d9a3c93-ae22-47ee-d2b8-37aa1cba0f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Последовательная реализация:\n",
            "Минимум: 1\n",
            "Максимум: 100\n",
            "Время выполнения: 0.019 мс\n",
            "\n",
            "Параллельная реализация (OpenMP):\n",
            "Минимум: 1\n",
            "Максимум: 100\n",
            "Время выполнения: 0.185 мс\n"
          ]
        }
      ],
      "source": [
        "!g++-15 -fopenmp task2.cpp -o task2\n",
        "!./task2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "350f3545",
      "metadata": {
        "id": "350f3545"
      },
      "source": [
        "### Сравнение времени выполнения и выводы\n",
        "\n",
        "        В ходе выполнения программы были получены следующие результаты:\n",
        "        последовательная реализация выполнила поиск минимального и максимального элементов за 0.019 мс,\n",
        "        в то время как параллельная реализация с использованием OpenMP заняла 0.185 мс.\n",
        "\n",
        "        Несмотря на использование параллельной обработки, параллельная версия оказалась медленнее последовательной.\n",
        "        Это объясняется тем, что при небольшом размере массива (10 000 элементов) накладные расходы на создание и\n",
        "        синхронизацию потоков превышают выигрыш от распараллеливания вычислений.\n",
        "\n",
        "        Таким образом, для небольших объёмов данных последовательная реализация является более эффективной.\n",
        "        Параллельная обработка с использованием OpenMP начинает показывать преимущество при значительно больших размерах массива,\n",
        "        когда вычислительная нагрузка компенсирует дополнительные затраты на управление потоками."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b1b313",
      "metadata": {
        "id": "d4b1b313"
      },
      "source": [
        "# Задача 3. Параллельная сортировка с OpenMP\n",
        "\n",
        "    Реализуйте алгоритм сортировки выбором с использованием OpenMP:\n",
        "        1. напишите последовательную реализацию алгоритма;\n",
        "        2. добавьте параллелизм с помощью директив OpenMP;\n",
        "        3. проверьте производительность для массивов размером 1 000 и 10 000 элементов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d30131ca-da2f-4fff-84c5-4de7b7be314a",
      "metadata": {
        "id": "d30131ca-da2f-4fff-84c5-4de7b7be314a",
        "outputId": "fe6e19d6-4652-4062-86fb-52ab2f9b0a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing task3.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile task3.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <cstdlib>      // для rand()\n",
        "#include <ctime>        // для time()\n",
        "#include <chrono>       // для измерения времени\n",
        "#include <omp.h>        // для OpenMP\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "// ==================== Последовательная сортировка выбором ====================\n",
        "// Проходит по массиву и выбирает минимальный элемент из неотсортированной части,\n",
        "// затем меняет его местами с текущим элементом.\n",
        "void selectionSortSequential(int* arr, int n) {\n",
        "    for (int i = 0; i < n - 1; i++) {\n",
        "        int min_idx = i;  // предполагаем, что минимальный элемент — текущий\n",
        "\n",
        "        // Поиск минимального элемента в оставшейся части массива\n",
        "        for (int j = i + 1; j < n; j++) {\n",
        "            if (arr[j] < arr[min_idx]) {\n",
        "                min_idx = j;  // обновляем индекс минимального элемента\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Обмен текущего элемента с найденным минимумом\n",
        "        swap(arr[i], arr[min_idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// ==================== Параллельная сортировка выбором (OpenMP) ====================\n",
        "// Основное ускорение достигается за счет параллельного поиска минимального элемента\n",
        "void selectionSortParallel(int* arr, int n) {\n",
        "    for (int i = 0; i < n - 1; i++) {\n",
        "        int min_idx = i;  // индекс глобального минимума\n",
        "\n",
        "        // Параллельный блок поиска минимального элемента\n",
        "#pragma omp parallel\n",
        "        {\n",
        "            int local_min_idx = min_idx;  // каждый поток имеет локальный минимум\n",
        "\n",
        "#pragma omp for nowait\n",
        "            // Потоки распределяют между собой итерации поиска минимума\n",
        "            for (int j = i + 1; j < n; j++) {\n",
        "                if (arr[j] < arr[local_min_idx]) {\n",
        "                    local_min_idx = j;  // обновление локального минимума\n",
        "                }\n",
        "            }\n",
        "\n",
        "            // Критическая секция для обновления глобального минимума\n",
        "#pragma omp critical\n",
        "            {\n",
        "                if (arr[local_min_idx] < arr[min_idx]) {\n",
        "                    min_idx = local_min_idx;  // обновление глобального минимума\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Обмен текущего элемента с глобальным минимумом\n",
        "        swap(arr[i], arr[min_idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int sizes[2] = {1000, 10000};  // размеры массивов для теста\n",
        "\n",
        "    srand(time(nullptr));  // инициализация генератора случайных чисел\n",
        "\n",
        "    for (int s = 0; s < 2; s++) {\n",
        "        int N = sizes[s];\n",
        "        cout << \"\\nРазмер массива: \" << N << endl;\n",
        "\n",
        "        // Выделение памяти под массивы для последовательной и параллельной сортировок\n",
        "        int* arr1 = new int[N];\n",
        "        int* arr2 = new int[N];\n",
        "\n",
        "        // Заполнение массивов случайными числами\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            arr1[i] = rand() % 10000;  // случайное число от 0 до 9999\n",
        "            arr2[i] = arr1[i];          // копия для параллельной сортировки\n",
        "        }\n",
        "\n",
        "        // ==================== Последовательная версия ====================\n",
        "        auto start_seq = high_resolution_clock::now();  // старт таймера\n",
        "        selectionSortSequential(arr1, N);               // вызов сортировки\n",
        "        auto end_seq = high_resolution_clock::now();    // конец таймера\n",
        "\n",
        "        double time_seq = duration<double, milli>(end_seq - start_seq).count(); // вычисление времени\n",
        "\n",
        "        // ==================== Параллельная версия ====================\n",
        "        auto start_par = high_resolution_clock::now();  // старт таймера\n",
        "        selectionSortParallel(arr2, N);                 // вызов параллельной сортировки\n",
        "        auto end_par = high_resolution_clock::now();    // конец таймера\n",
        "\n",
        "        double time_par = duration<double, milli>(end_par - start_par).count(); // вычисление времени\n",
        "\n",
        "        // ==================== Вывод результатов ====================\n",
        "        cout << \"Последовательная сортировка: \" << time_seq << \" мс\\n\";\n",
        "        cout << \"Параллельная сортировка (OpenMP): \" << time_par << \" мс\\n\";\n",
        "\n",
        "        // Освобождение памяти\n",
        "        delete[] arr1;\n",
        "        delete[] arr2;\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b057f14-5cf4-48f2-9003-6cd4987951e6",
      "metadata": {
        "id": "0b057f14-5cf4-48f2-9003-6cd4987951e6",
        "outputId": "281a8787-eb23-4f42-aea8-038819bc5dcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Размер массива: 1000\n",
            "Последовательная сортировка: 0.937 мс\n",
            "Параллельная сортировка (OpenMP): 50.22 мс\n",
            "\n",
            "Размер массива: 10000\n",
            "Последовательная сортировка: 43.875 мс\n",
            "Параллельная сортировка (OpenMP): 327.098 мс\n"
          ]
        }
      ],
      "source": [
        "!g++-15 -fopenmp task3.cpp -o task3\n",
        "!./task3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e7152a3",
      "metadata": {
        "id": "1e7152a3"
      },
      "source": [
        "# Задача 4. Сортировка на GPU с использованием CUDA\n",
        "\n",
        "    Реализуйте параллельную сортировку слиянием на GPU с использованием CUDA:\n",
        "\n",
        "        1. разделите массив на подмассивы, каждый из которых обрабатывается отдельным блоком;\n",
        "        2. выполните параллельное слияние отсортированных подмассивов;\n",
        "        3. замерьте производительность для массивов размером 10 000 и 100 000 элементов."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGiDW5D_UX3k",
        "outputId": "dc4c7ec7-73eb-451b-9514-8390b1eca1f7"
      },
      "id": "tGiDW5D_UX3k",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec 26 15:54:19 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task4.cu\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "#include <algorithm>\n",
        "#include <climits>\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "#define BLOCK_SIZE 256  // Количество потоков в одном блоке\n",
        "\n",
        "// ==================== Kernel 1: сортировка блока ====================\n",
        "// Каждый блок сортирует свой подмассив в shared memory (быстрая локальная память GPU)\n",
        "__global__ void blockSort(int* data, int n) {\n",
        "    __shared__ int shared[BLOCK_SIZE];  // shared memory для подмассива блока\n",
        "\n",
        "    int tid = threadIdx.x;                              // локальный индекс потока в блоке\n",
        "    int gid = blockIdx.x * blockDim.x + tid;           // глобальный индекс потока в массиве\n",
        "\n",
        "    // Копирование данных из глобальной памяти в shared memory\n",
        "    if (gid < n)\n",
        "        shared[tid] = data[gid];\n",
        "    else\n",
        "        shared[tid] = INT_MAX;  // если поток выходит за пределы массива, заполняем большим числом\n",
        "\n",
        "    __syncthreads();  // ждем, пока все потоки блока скопируют данные\n",
        "\n",
        "    // Простая сортировка пузырьком внутри блока\n",
        "    for (int i = 0; i < blockDim.x; i++) {\n",
        "        for (int j = tid; j < blockDim.x - 1; j += blockDim.x) {\n",
        "            if (shared[j] > shared[j + 1]) {\n",
        "                // Обмен значений\n",
        "                int tmp = shared[j];\n",
        "                shared[j] = shared[j + 1];\n",
        "                shared[j + 1] = tmp;\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();  // синхронизируем потоки после каждой итерации\n",
        "    }\n",
        "\n",
        "    // Копируем отсортированный блок обратно в глобальную память\n",
        "    if (gid < n)\n",
        "        data[gid] = shared[tid];\n",
        "}\n",
        "\n",
        "// ==================== Kernel 2: слияние подмассивов ====================\n",
        "// Каждый поток сливает два отсортированных подмассива заданной ширины\n",
        "__global__ void mergeKernel(int* input, int* output, int width, int n) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;  // индекс потока\n",
        "\n",
        "    int start = idx * 2 * width;      // начало первого подмассива\n",
        "    if (start >= n) return;           // если поток вне массива, выходим\n",
        "\n",
        "    int mid = min(start + width, n);      // конец первого подмассива и начало второго\n",
        "    int end = min(start + 2 * width, n);  // конец второго подмассива\n",
        "\n",
        "    int i = start;  // индекс для первого подмассива\n",
        "    int j = mid;    // индекс для второго подмассива\n",
        "    int k = start;  // индекс для записи в выходной массив\n",
        "\n",
        "    // Слияние двух подмассивов в отсортированный порядок\n",
        "    while (i < mid && j < end) {\n",
        "        if (input[i] <= input[j])\n",
        "            output[k++] = input[i++];\n",
        "        else\n",
        "            output[k++] = input[j++];\n",
        "    }\n",
        "\n",
        "    // Копирование оставшихся элементов первого подмассива\n",
        "    while (i < mid) output[k++] = input[i++];\n",
        "    // Копирование оставшихся элементов второго подмассива\n",
        "    while (j < end) output[k++] = input[j++];\n",
        "}\n",
        "\n",
        "// ==================== Хост-код ====================\n",
        "int main() {\n",
        "    vector<int> sizes = {10000, 100000};  // размеры массивов для теста\n",
        "\n",
        "    for (int N : sizes) {\n",
        "        vector<int> h_data(N);\n",
        "\n",
        "        // Заполняем массив случайными числами\n",
        "        for (int i = 0; i < N; i++)\n",
        "            h_data[i] = rand() % 1000000;\n",
        "\n",
        "        int* d_data;\n",
        "        int* d_temp;\n",
        "        cudaMalloc(&d_data, N * sizeof(int));  // выделяем память на GPU для основного массива\n",
        "        cudaMalloc(&d_temp, N * sizeof(int));  // выделяем память для временного массива\n",
        "\n",
        "        // Копируем массив с хоста на GPU\n",
        "        cudaMemcpy(d_data, h_data.data(), N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "        auto start = high_resolution_clock::now();  // старт таймера\n",
        "\n",
        "        // -------------------- Шаг 1: сортировка блоков --------------------\n",
        "        int numBlocks = (N + BLOCK_SIZE - 1) / BLOCK_SIZE;  // количество блоков\n",
        "        blockSort<<<numBlocks, BLOCK_SIZE>>>(d_data, N);    // вызов kernel\n",
        "        cudaDeviceSynchronize();                             // ждем завершения всех потоков\n",
        "\n",
        "        // -------------------- Шаг 2: итеративное слияние --------------------\n",
        "        for (int width = BLOCK_SIZE; width < N; width *= 2) {\n",
        "            int mergeBlocks = (N + 2 * width - 1) / (2 * width);  // сколько блоков для слияния\n",
        "            mergeKernel<<<mergeBlocks, 1>>>(d_data, d_temp, width, N); // запуск слияния\n",
        "            cudaDeviceSynchronize();                               // ждем завершения\n",
        "            swap(d_data, d_temp);                                  // меняем указатели массивов\n",
        "        }\n",
        "\n",
        "        auto end = high_resolution_clock::now();  // конец таймера\n",
        "        chrono::duration<double, milli> elapsed = end - start;\n",
        "\n",
        "        // Копируем результат обратно на CPU\n",
        "        cudaMemcpy(h_data.data(), d_data, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Проверка, отсортирован ли массив\n",
        "        bool sorted = is_sorted(h_data.begin(), h_data.end());\n",
        "\n",
        "        // -------------------- Вывод результатов --------------------\n",
        "        cout << \"==============================\" << endl;\n",
        "        cout << \"Размер массива: \" << N << \" элементов\" << endl;\n",
        "        cout << \"Сортировка завершена: \" << (sorted ? \"Да\" : \"Нет\") << endl;\n",
        "        cout << \"Время выполнения на GPU: \" << elapsed.count() << \" мс\" << endl;\n",
        "\n",
        "        // Вывод первых и последних 5 элементов\n",
        "        cout << \"Первые 5 элементов: \";\n",
        "        for (int i = 0; i < min(5, N); i++) cout << h_data[i] << \" \";\n",
        "        cout << \"\\nПоследние 5 элементов: \";\n",
        "        for (int i = max(0, N - 5); i < N; i++) cout << h_data[i] << \" \";\n",
        "        cout << endl;\n",
        "\n",
        "        // Освобождение памяти GPU\n",
        "        cudaFree(d_data);\n",
        "        cudaFree(d_temp);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iEwBdSCXjTS",
        "outputId": "6a1dbb86-0cf9-4a22-86d8-56354badc2b8"
      },
      "id": "6iEwBdSCXjTS",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing task4.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc task4.cu -o task4\n",
        "!./task4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BooyM9MXri_",
        "outputId": "0a1fc4b9-cbbe-4211-952e-0ceb149088fe"
      },
      "id": "7BooyM9MXri_",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==============================\n",
            "Размер массива: 10000 элементов\n",
            "Сортировка завершена: Нет\n",
            "Время выполнения на GPU: 44.3669 мс\n",
            "Первые 5 элементов: 289383 930886 692777 636915 747793 \n",
            "Последние 5 элементов: 287797 480021 920292 459307 609430 \n",
            "==============================\n",
            "Размер массива: 100000 элементов\n",
            "Сортировка завершена: Да\n",
            "Время выполнения на GPU: 0.045622 мс\n",
            "Первые 5 элементов: 0 0 0 0 0 \n",
            "Последние 5 элементов: 0 0 0 0 0 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GWRb2IgcX0qj"
      },
      "id": "GWRb2IgcX0qj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}