# Assignment 3

## Вывод по заданию 1

В первом задании была реализована поэлементная обработка массива размером **1 000 000 элементов** с использованием двух подходов работы с памятью GPU: **глобальной памяти** и **разделяемой (shared) памяти**.

При использовании только глобальной памяти время выполнения составило **54.08 мс**. В данном случае каждый поток напрямую обращается к глобальной памяти GPU, что сопровождается высокой задержкой доступа и, как следствие, снижением общей производительности программы.

Во второй реализации применялась **разделяемая память**, и время выполнения сократилось до **0.072 мс**. Это объясняется тем, что данные сначала загружаются в shared memory, которая имеет существенно меньшую задержку по сравнению с глобальной памятью. После этого все вычисления выполняются быстрее внутри блока потоков.

Полученные результаты наглядно демонстрируют, что использование разделяемой памяти способно обеспечить многократный прирост производительности при поэлементной обработке данных, особенно для массивов большого размера. Корректная организация работы с памятью является одним из ключевых факторов оптимизации CUDA-программ.


## Вывод по заданию 2

В рамках задания было исследовано влияние размера блока потоков на производительность CUDA-программы по поэлементному сложению двух массивов размером **1 000 000 элементов**.

При размере блока **128 потоков** время выполнения составило **7.73 мс**. Небольшой размер блока приводит к менее эффективному использованию вычислительных ресурсов GPU и большему числу блоков, что увеличивает накладные расходы на их обработку.

При увеличении размера блока до **256 потоков** время выполнения значительно сократилось до **0.0049 мс**. В этом случае достигается более эффективная загрузка мультипроцессоров GPU и лучше используется параллелизм архитектуры.

Наилучший результат был получен при размере блока **512 потоков** — **0.0016 мс**. Такой размер блока позволяет максимально эффективно задействовать аппаратные ресурсы, уменьшая накладные расходы и повышая пропускную способность обработки данных.

Таким образом, эксперимент показывает, что корректный выбор размера блока потоков критически важен для производительности CUDA-программ. Увеличение размера блока до оптимального значения позволяет добиться существенного ускорения вычислений по сравнению с неоптимальной конфигурацией.


## Вывод по заданию 3

Для массива размером **1 000 000 элементов** было проведено сравнение двух вариантов доступа к глобальной памяти GPU: **коалесцированного** и **некоалесцированного**.

При **коалесцированном доступе** время выполнения составило **7.63 мс**. В данном случае потоки одного варпа обращаются к соседним ячейкам памяти, что позволяет GPU объединять обращения в меньшее количество транзакций. Такой шаблон доступа считается корректным и рекомендуется для эффективной работы с глобальной памятью.

При **некоалесцированном доступе** время выполнения составило **0.07 мс**. Несмотря на то, что данный способ обращения к памяти считается менее эффективным с точки зрения архитектуры GPU, в рамках данной реализации он показал меньшее время выполнения. Это может быть связано с простотой выполняемой операции, особенностями тестового CUDA-ядра, а также влиянием накладных расходов на запуск ядра и оптимизаций компилятора.

В целом эксперимент демонстрирует, что способ обращения к глобальной памяти напрямую влияет на производительность CUDA-программ. Однако для корректной интерпретации результатов необходимо учитывать характер вычислений, объём реальной работы внутри ядра и вклад накладных расходов.


## Вывод по заданию 4

Для массива размером **1 000 000 элементов** было показано, что выбор конфигурации сетки и блоков потоков существенно влияет на производительность CUDA-программы.

При **неоптимальной конфигурации** (64 потока на блок) время выполнения составило **11.13 мс**. Такое время объясняется низкой загрузкой вычислительных ресурсов GPU, недостаточным количеством активных потоков и менее эффективным использованием аппаратного параллелизма.

При **оптимальной конфигурации** (256 потоков на блок) время выполнения снизилось до **0.0029 мс**. В этом случае достигается более полное использование потоковых мультипроцессоров, лучше скрываются задержки доступа к памяти и уменьшаются накладные расходы на запуск блоков.

Таким образом, корректный подбор параметров сетки и блоков потоков является критически важным этапом оптимизации CUDA-программ и может обеспечивать прирост производительности на порядки.


# Контрольные вопросы к Assignment 3

## 1. Какие основные типы памяти существуют в архитектуре CUDA и чем они отличаются по скорости доступа?
В архитектуре CUDA используются несколько типов памяти:
- **Глобальная память** — имеет большой объём и доступна всем потокам, но обладает высокой задержкой доступа.
- **Разделяемая память (Shared Memory)** — общая для потоков одного блока, значительно быстрее глобальной памяти.
- **Регистры** — самая быстрая память, локальная для каждого потока, но сильно ограничена по объёму.
- **Локальная память** — используется при нехватке регистров, физически размещается в глобальной памяти и является медленной.
- **Константная и текстурная память** — оптимизированы для операций чтения и эффективны при определённых шаблонах доступа.

---

## 2. В каких случаях использование разделяемой памяти позволяет ускорить выполнение CUDA-программы?
Использование разделяемой памяти эффективно, когда:
- данные используются несколькими потоками одного блока;
- требуется сократить количество обращений к глобальной памяти;
- выполняются операции над соседними элементами массива;
- данные повторно используются в вычислениях.

---

## 3. Как шаблон доступа к глобальной памяти влияет на производительность GPU-программы?
Производительность GPU существенно зависит от шаблона доступа к памяти:
- **Коалесцированный доступ** позволяет потокам варпа обращаться к последовательным адресам памяти, минимизируя задержки.
- **Некоалесцированный доступ** приводит к большему числу транзакций памяти и снижению производительности.

---

## 4. Почему одинаковый алгоритм на GPU может показывать разное время выполнения при разных способах обращения к памяти?
Даже при одинаковом алгоритме время выполнения может отличаться из-за:
- разной задержки доступа к памяти;
- количества обращений к глобальной памяти;
- отсутствия или наличия использования shared memory;
- коалесцированного или некоалесцированного доступа.

---

## 5. Как размер блока потоков влияет на производительность CUDA-ядра?
Размер блока потоков влияет на:
- степень загрузки GPU (occupancy);
- эффективность использования варпов;
- объём доступных регистров и shared memory;
- баланс между параллелизмом и использованием ресурсов.

---

## 6. Что такое варп и почему важно учитывать его при разработке CUDA-программ?
**Варп** — это группа из 32 потоков, выполняющихся одновременно.  
Учет варпов важен, потому что:
- все потоки варпа выполняют одну инструкцию;
- ветвление внутри варпа снижает производительность;
- коалесцированный доступ к памяти формируется на уровне варпа.

---

## 7. Какие факторы необходимо учитывать при выборе конфигурации сетки и блоков потоков?
При выборе конфигурации необходимо учитывать:
- размер обрабатываемых данных;
- архитектуру GPU;
- размер варпа;
- использование регистров;
- объём shared memory;
- достижение высокой загрузки вычислительных блоков.

---

## 8. Почему оптимизация CUDA-программы часто начинается с анализа работы с памятью, а не с изменения алгоритма?
Потому что в большинстве CUDA-программ основным узким местом является память.  
Оптимизация доступа к памяти позволяет значительно ускорить выполнение программы без изменения логики алгоритма.