{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw2t08bqtr8O",
        "outputId": "42b18e1a-12cd-48a3-b17f-64b6f812d5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduction_kernel.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile reduction_kernel.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "\n",
        "#define THREADS 256\n",
        "\n",
        "// ======================\n",
        "// Reduction kernel with shared memory\n",
        "// ======================\n",
        "__global__ void reduce_sum(const float* input, float* output, int N) {\n",
        "    __shared__ float sdata[THREADS];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load element into shared memory\n",
        "    sdata[tid] = (idx < N) ? input[idx] : 0.0f;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduction in shared memory\n",
        "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s) sdata[tid] += sdata[tid + s];\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write result of this block\n",
        "    if (tid == 0) output[blockIdx.x] = sdata[0];\n",
        "}\n",
        "\n",
        "// ======================\n",
        "// Host code\n",
        "// ======================\n",
        "int main() {\n",
        "    const int N = 1 << 20; // 1M elements\n",
        "    std::vector<float> h_input(N, 1.0f);\n",
        "    int blocks = (N + THREADS - 1) / THREADS;\n",
        "\n",
        "    float *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, N * sizeof(float));\n",
        "    cudaMalloc(&d_output, blocks * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_input, h_input.data(), N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "    reduce_sum<<<blocks, THREADS>>>(d_input, d_output, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Final reduction on CPU\n",
        "    std::vector<float> h_partial(blocks);\n",
        "    cudaMemcpy(h_partial.data(), d_output, blocks * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    float sum = 0.0f;\n",
        "    for (auto v : h_partial) sum += v;\n",
        "\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::cout << \"CUDA Reduction Sum: \" << sum << \"\\n\";\n",
        "    std::cout << \"Time: \" << std::chrono::duration<double, std::milli>(end - start).count() << \" ms\\n\";\n",
        "\n",
        "    // CPU sum for comparison\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    float cpu_sum = 0.0f;\n",
        "    for (auto v : h_input) cpu_sum += v;\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::cout << \"CPU Sum: \" << cpu_sum << \"\\n\";\n",
        "    std::cout << \"CPU Time: \" << std::chrono::duration<double, std::milli>(cpu_end - cpu_start).count() << \" ms\\n\";\n",
        "\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scan_kernel.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "\n",
        "#define THREADS 256\n",
        "\n",
        "// ======================\n",
        "// Prefix sum (exclusive scan) with shared memory\n",
        "// ======================\n",
        "__global__ void scan_kernel(float* data, int N) {\n",
        "    __shared__ float temp[THREADS];\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    // Load data into shared memory\n",
        "    temp[tid] = (idx < N) ? data[idx] : 0.0f;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Up-sweep / reduce phase\n",
        "    for (int offset = 1; offset < blockDim.x; offset *= 2) {\n",
        "        int ai = (tid + 1) * offset * 2 - 1;\n",
        "        int bi = ai - offset;\n",
        "        if (ai < THREADS) temp[ai] += temp[bi];\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Clear last element for exclusive scan\n",
        "    if (tid == 0) temp[THREADS - 1] = 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Down-sweep phase\n",
        "    for (int offset = THREADS / 2; offset > 0; offset /= 2) {\n",
        "        int ai = (tid + 1) * offset * 2 - 1;\n",
        "        int bi = ai - offset;\n",
        "        if (ai < THREADS) {\n",
        "            float t = temp[bi];\n",
        "            temp[bi] = temp[ai];\n",
        "            temp[ai] += t;\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write results back\n",
        "    if (idx < N) data[idx] = temp[tid];\n",
        "}\n",
        "\n",
        "// ======================\n",
        "// Host code\n",
        "// ======================\n",
        "int main() {\n",
        "    const int N = 1 << 16; // smaller array for scan\n",
        "    std::vector<float> h_data(N, 1.0f);\n",
        "\n",
        "    float* d_data;\n",
        "    cudaMalloc(&d_data, N * sizeof(float));\n",
        "    cudaMemcpy(d_data, h_data.data(), N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "    scan_kernel<<<(N + THREADS - 1)/THREADS, THREADS>>>(d_data, N);\n",
        "    cudaDeviceSynchronize();\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    cudaMemcpy(h_data.data(), d_data, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Check correctness (sum of last element + 1 should equal total sum)\n",
        "    float total = 0.0f;\n",
        "    for (int i = 0; i < N; i++) total += 1.0f; // CPU sum for reference\n",
        "    std::cout << \"Last element after scan: \" << h_data[N-1] << \"\\n\";\n",
        "    std::cout << \"Total sum (CPU): \" << total << \"\\n\";\n",
        "    std::cout << \"CUDA Scan Time: \" << std::chrono::duration<double, std::milli>(end - start).count() << \" ms\\n\";\n",
        "\n",
        "    cudaFree(d_data);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cITS9U0tvs6",
        "outputId": "36392427-d80d-4ab3-f8f1-138e530cdc4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scan_kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction_kernel.cu -o reduction\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMi8QGOgtv8M",
        "outputId": "0838f014-8a4f-4f10-a4cf-49b9dcb2ad46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Reduction Sum: 0\n",
            "Time: 42.9607 ms\n",
            "CPU Sum: 1.04858e+06\n",
            "CPU Time: 11.673 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc scan_kernel.cu -o scan\n",
        "!./scan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeWNf8umtzWF",
        "outputId": "1520e538-4c73-4d57-9d95-e56453d0ba0d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last element after scan: 1\n",
            "Total sum (CPU): 65536\n",
            "CUDA Scan Time: 8.79022 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Control Questions – Reduction and Scan in CUDA\n",
        "\n",
        "### 1. What is the difference between reduction and scan?\n",
        "\n",
        "- **Reduction**  \n",
        "  - Combines all elements of an array into a single value using an associative operation (e.g., sum, max, min).  \n",
        "  - Example: summing all elements of an array → result is a single number.\n",
        "\n",
        "- **Scan (Prefix Sum)**  \n",
        "  - Computes all the intermediate sums (or other associative operations) of an array.  \n",
        "  - Produces an array of the same size as input, where each element represents the sum of all previous elements.  \n",
        "  - Example: exclusive sum of `[1, 2, 3, 4]` → `[0, 1, 3, 6]`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Which CUDA memory types are used to optimize reduction and scan?\n",
        "\n",
        "- **Shared memory**  \n",
        "  - Used to store intermediate results within a block.  \n",
        "  - Reduces slow accesses to global memory.  \n",
        "  - Enables efficient parallel computation and synchronization among threads in a block.\n",
        "\n",
        "- **Global memory**  \n",
        "  - Used for storing the original array and final output.  \n",
        "  - Access should be minimized during computation for better performance.\n",
        "\n",
        "- **Registers / Private memory**  \n",
        "  - Used for temporary variables within threads.  \n",
        "  - Very fast, limited capacity.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. How can a prefix sum be optimized on GPU?\n",
        "\n",
        "- Use **shared memory** to hold partial sums within a block.  \n",
        "- Apply **work-efficient scan algorithms**: up-sweep and down-sweep phases.  \n",
        "- Use **multiple blocks** with hierarchical scan:\n",
        "  1. Perform scan within each block.  \n",
        "  2. Scan the block sums on the CPU or another kernel.  \n",
        "  3. Add scanned block sums back to each block’s result.  \n",
        "- Avoid unnecessary global memory accesses and minimize thread divergence.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Give an example of a problem where scan is used\n",
        "\n",
        "- **Stream compaction** – removing unwanted elements (e.g., zeros) from an array.  \n",
        "- **Histogram computation** – calculating cumulative distribution.  \n",
        "- **Prefix sums in parallel algorithms** – e.g., parallel radix sort, parallel BFS, or GPU sorting algorithms.  \n",
        "- **Particle simulations** – computing indices for particle reordering in physics simulations."
      ],
      "metadata": {
        "id": "RwH-0_4cudTd"
      }
    }
  ]
}