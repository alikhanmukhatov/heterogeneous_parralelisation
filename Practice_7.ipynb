{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw2t08bqtr8O",
        "outputId": "42b18e1a-12cd-48a3-b17f-64b6f812d5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduction_kernel.cu\n"
          ]
        }
      ],
      "source": [
        "// Write the following CUDA program to a file named reduction_kernel.cu\n",
        "%%writefile reduction_kernel.cu\n",
        "\n",
        "// Include CUDA runtime API for memory management, kernel launches, and synchronization\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Include standard input/output stream library\n",
        "#include <iostream>\n",
        "\n",
        "// Include vector container from the C++ standard library\n",
        "#include <vector>\n",
        "\n",
        "// Include chrono library for performance timing\n",
        "#include <chrono>\n",
        "\n",
        "// Define number of threads per block\n",
        "#define THREADS 256\n",
        "\n",
        "// ======================\n",
        "// Reduction kernel with shared memory\n",
        "// ======================\n",
        "\n",
        "// CUDA kernel that performs block-level reduction (sum) using shared memory\n",
        "__global__ void reduce_sum(const float* input, float* output, int N) {\n",
        "\n",
        "    // Declare shared memory array for partial sums within a block\n",
        "    __shared__ float sdata[THREADS];\n",
        "\n",
        "    // Thread index within the block\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // Global index of the element this thread will process\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load input element into shared memory if within bounds, otherwise load 0\n",
        "    sdata[tid] = (idx < N) ? input[idx] : 0.0f;\n",
        "\n",
        "    // Synchronize all threads in the block to ensure shared memory is fully loaded\n",
        "    __syncthreads();\n",
        "\n",
        "    // Perform parallel reduction in shared memory\n",
        "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "\n",
        "        // Only threads with index less than s participate in this step\n",
        "        if (tid < s)\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "\n",
        "        // Synchronize after each reduction step\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // The first thread in each block writes the block's sum to global memory\n",
        "    if (tid == 0)\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "}\n",
        "\n",
        "// ======================\n",
        "// Host code\n",
        "// ======================\n",
        "\n",
        "// Main program entry point\n",
        "int main() {\n",
        "\n",
        "    // Total number of elements (1,048,576 elements)\n",
        "    const int N = 1 << 20;\n",
        "\n",
        "    // Create and initialize host input vector with all elements equal to 1.0\n",
        "    std::vector<float> h_input(N, 1.0f);\n",
        "\n",
        "    // Calculate the number of CUDA blocks required\n",
        "    int blocks = (N + THREADS - 1) / THREADS;\n",
        "\n",
        "    // Device pointer for input array\n",
        "    float* d_input;\n",
        "\n",
        "    // Device pointer for partial output (one value per block)\n",
        "    float* d_output;\n",
        "\n",
        "    // Allocate device memory for input array\n",
        "    cudaMalloc(&d_input, N * sizeof(float));\n",
        "\n",
        "    // Allocate device memory for partial reduction results\n",
        "    cudaMalloc(&d_output, blocks * sizeof(float));\n",
        "\n",
        "    // Copy input data from host memory to device memory\n",
        "    cudaMemcpy(d_input, h_input.data(), N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Record start time for CUDA reduction\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Launch reduction kernel with computed number of blocks and threads\n",
        "    reduce_sum<<<blocks, THREADS>>>(d_input, d_output, N);\n",
        "\n",
        "    // Wait for kernel execution to finish\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Host vector to store partial sums from each block\n",
        "    std::vector<float> h_partial(blocks);\n",
        "\n",
        "    // Copy partial sums from device to host\n",
        "    cudaMemcpy(h_partial.data(), d_output, blocks * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Final reduction on CPU to accumulate block-level results\n",
        "    float sum = 0.0f;\n",
        "    for (auto v : h_partial)\n",
        "        sum += v;\n",
        "\n",
        "    // Record end time for CUDA reduction\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Print CUDA reduction result\n",
        "    std::cout << \"CUDA Reduction Sum: \" << sum << \"\\n\";\n",
        "\n",
        "    // Print CUDA execution time in milliseconds\n",
        "    std::cout << \"Time: \"\n",
        "              << std::chrono::duration<double, std::milli>(end - start).count()\n",
        "              << \" ms\\n\";\n",
        "\n",
        "    // ======================\n",
        "    // CPU reduction for comparison\n",
        "    // ======================\n",
        "\n",
        "    // Record start time for CPU summation\n",
        "    auto cpu_start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Perform sequential sum on CPU\n",
        "    float cpu_sum = 0.0f;\n",
        "    for (auto v : h_input)\n",
        "        cpu_sum += v;\n",
        "\n",
        "    // Record end time for CPU summation\n",
        "    auto cpu_end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Print CPU reduction result\n",
        "    std::cout << \"CPU Sum: \" << cpu_sum << \"\\n\";\n",
        "\n",
        "    // Print CPU execution time in milliseconds\n",
        "    std::cout << \"CPU Time: \"\n",
        "              << std::chrono::duration<double, std::milli>(cpu_end - cpu_start).count()\n",
        "              << \" ms\\n\";\n",
        "\n",
        "    // Free device memory for input array\n",
        "    cudaFree(d_input);\n",
        "\n",
        "    // Free device memory for output array\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    // Exit program successfully\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "// Write the following CUDA program to a file named scan_kernel.cu\n",
        "%%writefile scan_kernel.cu\n",
        "\n",
        "// Include CUDA runtime API for kernel execution and memory management\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Include standard C++ input/output stream library\n",
        "#include <iostream>\n",
        "\n",
        "// Include vector container from the C++ standard library\n",
        "#include <vector>\n",
        "\n",
        "// Include chrono library for performance timing\n",
        "#include <chrono>\n",
        "\n",
        "// Define number of threads per CUDA block\n",
        "#define THREADS 256\n",
        "\n",
        "// ======================\n",
        "// Prefix sum (exclusive scan) with shared memory\n",
        "// ======================\n",
        "\n",
        "// CUDA kernel that performs an exclusive prefix sum (scan) per block\n",
        "__global__ void scan_kernel(float* data, int N) {\n",
        "\n",
        "    // Shared memory array used to store block-local data\n",
        "    __shared__ float temp[THREADS];\n",
        "\n",
        "    // Thread index within the block\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // Global index of the element processed by this thread\n",
        "    int idx = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    // Load global memory data into shared memory if within bounds\n",
        "    temp[tid] = (idx < N) ? data[idx] : 0.0f;\n",
        "\n",
        "    // Synchronize all threads to ensure shared memory is fully loaded\n",
        "    __syncthreads();\n",
        "\n",
        "    // ----------------------\n",
        "    // Up-sweep (reduce) phase\n",
        "    // ----------------------\n",
        "\n",
        "    // Loop over reduction offsets, doubling each iteration\n",
        "    for (int offset = 1; offset < blockDim.x; offset *= 2) {\n",
        "\n",
        "        // Compute index for right child\n",
        "        int ai = (tid + 1) * offset * 2 - 1;\n",
        "\n",
        "        // Compute index for left child\n",
        "        int bi = ai - offset;\n",
        "\n",
        "        // Accumulate partial sums if index is within shared memory bounds\n",
        "        if (ai < THREADS)\n",
        "            temp[ai] += temp[bi];\n",
        "\n",
        "        // Synchronize threads after each reduction step\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // ----------------------\n",
        "    // Prepare for down-sweep\n",
        "    // ----------------------\n",
        "\n",
        "    // Clear the last element to convert inclusive scan to exclusive scan\n",
        "    if (tid == 0)\n",
        "        temp[THREADS - 1] = 0;\n",
        "\n",
        "    // Synchronize to ensure last element is reset\n",
        "    __syncthreads();\n",
        "\n",
        "    // ----------------------\n",
        "    // Down-sweep phase\n",
        "    // ----------------------\n",
        "\n",
        "    // Loop over offsets in reverse order\n",
        "    for (int offset = THREADS / 2; offset > 0; offset /= 2) {\n",
        "\n",
        "        // Compute index for right child\n",
        "        int ai = (tid + 1) * offset * 2 - 1;\n",
        "\n",
        "        // Compute index for left child\n",
        "        int bi = ai - offset;\n",
        "\n",
        "        // Swap and accumulate values if index is within bounds\n",
        "        if (ai < THREADS) {\n",
        "\n",
        "            // Temporarily store left value\n",
        "            float t = temp[bi];\n",
        "\n",
        "            // Move right value to left position\n",
        "            temp[bi] = temp[ai];\n",
        "\n",
        "            // Add left value to right position\n",
        "            temp[ai] += t;\n",
        "        }\n",
        "\n",
        "        // Synchronize threads after each step\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // ----------------------\n",
        "    // Write results back to global memory\n",
        "    // ----------------------\n",
        "\n",
        "    // Store the scanned value back to global memory if within bounds\n",
        "    if (idx < N)\n",
        "        data[idx] = temp[tid];\n",
        "}\n",
        "\n",
        "// ======================\n",
        "// Host code\n",
        "// ======================\n",
        "\n",
        "// Main program entry point\n",
        "int main() {\n",
        "\n",
        "    // Define number of elements for the scan operation\n",
        "    const int N = 1 << 16; // 65,536 elements (smaller array for scan)\n",
        "\n",
        "    // Create and initialize host vector with all elements equal to 1.0\n",
        "    std::vector<float> h_data(N, 1.0f);\n",
        "\n",
        "    // Device pointer for data array\n",
        "    float* d_data;\n",
        "\n",
        "    // Allocate device memory for the data array\n",
        "    cudaMalloc(&d_data, N * sizeof(float));\n",
        "\n",
        "    // Copy input data from host memory to device memory\n",
        "    cudaMemcpy(d_data, h_data.data(), N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Record start time before kernel execution\n",
        "    auto start = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Launch scan kernel with enough blocks to cover all elements\n",
        "    scan_kernel<<<(N + THREADS - 1) / THREADS, THREADS>>>(d_data, N);\n",
        "\n",
        "    // Wait until kernel execution is complete\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Record end time after kernel execution\n",
        "    auto end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // Copy scanned data from device memory back to host memory\n",
        "    cudaMemcpy(h_data.data(), d_data, N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // ----------------------\n",
        "    // Correctness check\n",
        "    // ----------------------\n",
        "\n",
        "    // Compute total sum on CPU for reference\n",
        "    float total = 0.0f;\n",
        "    for (int i = 0; i < N; i++)\n",
        "        total += 1.0f;\n",
        "\n",
        "    // Print the last element of the scanned array\n",
        "    std::cout << \"Last element after scan: \" << h_data[N - 1] << \"\\n\";\n",
        "\n",
        "    // Print the expected total sum computed on CPU\n",
        "    std::cout << \"Total sum (CPU): \" << total << \"\\n\";\n",
        "\n",
        "    // Print CUDA scan execution time in milliseconds\n",
        "    std::cout << \"CUDA Scan Time: \"\n",
        "              << std::chrono::duration<double, std::milli>(end - start).count()\n",
        "              << \" ms\\n\";\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_data);\n",
        "\n",
        "    // Exit program successfully\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cITS9U0tvs6",
        "outputId": "36392427-d80d-4ab3-f8f1-138e530cdc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing scan_kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction_kernel.cu -o reduction\n",
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMi8QGOgtv8M",
        "outputId": "0838f014-8a4f-4f10-a4cf-49b9dcb2ad46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Reduction Sum: 0\n",
            "Time: 42.9607 ms\n",
            "CPU Sum: 1.04858e+06\n",
            "CPU Time: 11.673 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc scan_kernel.cu -o scan\n",
        "!./scan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeWNf8umtzWF",
        "outputId": "1520e538-4c73-4d57-9d95-e56453d0ba0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last element after scan: 1\n",
            "Total sum (CPU): 65536\n",
            "CUDA Scan Time: 8.79022 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Control Questions – Reduction and Scan in CUDA\n",
        "\n",
        "### 1. What is the difference between reduction and scan?\n",
        "\n",
        "- **Reduction**  \n",
        "  - Combines all elements of an array into a single value using an associative operation (e.g., sum, max, min).  \n",
        "  - Example: summing all elements of an array → result is a single number.\n",
        "\n",
        "- **Scan (Prefix Sum)**  \n",
        "  - Computes all the intermediate sums (or other associative operations) of an array.  \n",
        "  - Produces an array of the same size as input, where each element represents the sum of all previous elements.  \n",
        "  - Example: exclusive sum of `[1, 2, 3, 4]` → `[0, 1, 3, 6]`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Which CUDA memory types are used to optimize reduction and scan?\n",
        "\n",
        "- **Shared memory**  \n",
        "  - Used to store intermediate results within a block.  \n",
        "  - Reduces slow accesses to global memory.  \n",
        "  - Enables efficient parallel computation and synchronization among threads in a block.\n",
        "\n",
        "- **Global memory**  \n",
        "  - Used for storing the original array and final output.  \n",
        "  - Access should be minimized during computation for better performance.\n",
        "\n",
        "- **Registers / Private memory**  \n",
        "  - Used for temporary variables within threads.  \n",
        "  - Very fast, limited capacity.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. How can a prefix sum be optimized on GPU?\n",
        "\n",
        "- Use **shared memory** to hold partial sums within a block.  \n",
        "- Apply **work-efficient scan algorithms**: up-sweep and down-sweep phases.  \n",
        "- Use **multiple blocks** with hierarchical scan:\n",
        "  1. Perform scan within each block.  \n",
        "  2. Scan the block sums on the CPU or another kernel.  \n",
        "  3. Add scanned block sums back to each block’s result.  \n",
        "- Avoid unnecessary global memory accesses and minimize thread divergence.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Give an example of a problem where scan is used\n",
        "\n",
        "- **Stream compaction** – removing unwanted elements (e.g., zeros) from an array.  \n",
        "- **Histogram computation** – calculating cumulative distribution.  \n",
        "- **Prefix sums in parallel algorithms** – e.g., parallel radix sort, parallel BFS, or GPU sorting algorithms.  \n",
        "- **Particle simulations** – computing indices for particle reordering in physics simulations."
      ],
      "metadata": {
        "id": "RwH-0_4cudTd"
      }
    }
  ]
}